import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
from mlxtend.frequent_patterns import apriori, association_rules
from mlxtend.preprocessing import TransactionEncoder

# âœ… í•œê¸€ í°íŠ¸ ì„¤ì • (macOSìš©)
plt.rcParams['font.family'] = 'AppleGothic'
plt.rcParams['axes.unicode_minus'] = False

# âœ… 1. íŒŒì¼ ê²½ë¡œ ì…ë ¥ (ì‹¤ì œ ìœ„ì¹˜ì— ë§ê²Œ ìˆ˜ì •!)
file_path = "/Users/jsh/Desktop/ë°ì´í„° ë§ˆì´ë‹/ì‚°ë¦¼ì²­ êµ­ë¦½ì‚°ë¦¼ê³¼í•™ì›_ëŒ€í˜•ì‚°ë¶ˆìœ„í—˜ì˜ˆë³´ëª©ë¡ì •ë³´_20250228.csv"
data = pd.read_csv(file_path, encoding="cp949")

# âœ… 2. ìœ„í—˜ë„ êµ¬ê°„í™” í•¨ìˆ˜ ì •ì˜
def classify_risk(row):
    if row['ì‹¤íš¨ìŠµë„'] < 30 and row['í’ì†'] > 7:
        return 'ë†’ìŒ'
    elif 30 <= row['ì‹¤íš¨ìŠµë„'] < 40 and 5 < row['í’ì†'] <= 7:
        return 'ë³´í†µ'
    else:
        return 'ë‚®ìŒ'

# âœ… 3. ìœ„í—˜ë„ ì»¬ëŸ¼ ìƒì„±
data['ìœ„í—˜ë„'] = data.apply(classify_risk, axis=1)
print("ğŸ“Œ ìœ„í—˜ë„ ë¶„í¬:\n", data['ìœ„í—˜ë„'].value_counts())

# âœ… 4. ë°ì´í„° ë¶„í•  ë° ëª¨ë¸ í•™ìŠµ
X = data[['ì‹¤íš¨ìŠµë„', 'í’ì†']]
y = data['ìœ„í—˜ë„']
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# âœ… 5. ì˜ˆì¸¡ ë° í‰ê°€
y_pred = model.predict(X_test)
print("\nğŸ“Š Classification Report:\n", classification_report(y_test, y_pred))
print("\nğŸ”€ Confusion Matrix:\n", confusion_matrix(y_test, y_pred))

# âœ… 6. í˜¼ë™ í–‰ë ¬ ì‹œê°í™”
sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d',
            xticklabels=model.classes_, yticklabels=model.classes_, cmap='Blues')
plt.xlabel("ì˜ˆì¸¡ ë“±ê¸‰")
plt.ylabel("ì‹¤ì œ ë“±ê¸‰")
plt.title("ğŸ”¥ ì‚°ë¶ˆ ìœ„í—˜ë„ ì˜ˆì¸¡ í˜¼ë™ í–‰ë ¬")
plt.tight_layout()
plt.show()

# âœ… 7. ì—°ê´€ ê·œì¹™ ë¶„ì„: ì˜ˆì¸¡ì´ 'ë†’ìŒ'ì¸ ì¡°ê±´ë§Œ
df = X.copy()
df['predicted_risk'] = y_pred
high_risk_df = df[df['predicted_risk'] == 'ë†’ìŒ'].drop(columns=['predicted_risk'])

# ì—°ì†í˜• ë°ì´í„°ë¥¼ ë²”ì£¼í˜•ìœ¼ë¡œ êµ¬ê°„í™”
for col in high_risk_df.columns:
    if high_risk_df[col].dtype != 'object':
        high_risk_df[col] = pd.qcut(high_risk_df[col], q=3, duplicates='drop').astype(str)

# ê±°ë˜ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
transactions = high_risk_df.astype(str).values.tolist()
te = TransactionEncoder()
te_ary = te.fit(transactions).transform(transactions)
df_trans = pd.DataFrame(te_ary, columns=te.columns_)

# Apriori ë° ì—°ê´€ ê·œì¹™ ë„ì¶œ
frequent_itemsets = apriori(df_trans, min_support=0.1, use_colnames=True)
rules = association_rules(frequent_itemsets, metric="confidence", min_threshold=0.7)

# âœ… 8. ê·œì¹™ ê²°ê³¼ ì¶œë ¥
print("\nğŸ“Œ ìƒìœ„ ì—°ê´€ ê·œì¹™ (lift ê¸°ì¤€):")
print(rules.sort_values(by='lift', ascending=False).head(10))